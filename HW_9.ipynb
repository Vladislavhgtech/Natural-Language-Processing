{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "HW9.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDkfRi4xmh9H"
      },
      "source": [
        "### Взять набор данных на ваше усмотрение (стихи/прозу) или что-то ещё для \n",
        "### примера можно так же использовать прикреплённый Евгений Онегин"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfzKGQCAmh9J"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue-eB4qBmh9K"
      },
      "source": [
        "PATH_TO_FILE = '/content/onegin.txt'\n",
        "\n",
        "with open(file=PATH_TO_FILE, mode='r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsVS4m3-mh9L"
      },
      "source": [
        "### 1. поэкспериментировать с посимвольным подходом\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l257vzGkmh9L"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "\n",
        "idx2char = np.array(vocab)\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(101, drop_remainder=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWuYvaZpmh9L",
        "outputId": "0000a98f-6a3b-4f74-c33e-11deb87f6e66"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('\\tInput data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('\\n\\tTarget data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tInput data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
            "\n",
            "\tTarget data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVbkS7ZPmh9M",
        "outputId": "31bf640c-b9cb-4d0b-9651-999501ef50c7"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10_000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOAmjsN0mh9N"
      },
      "source": [
        "embedding_dim = 256\n",
        "rnn_units = 512\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6VJtUZJmh9N"
      },
      "source": [
        "checkpoint_path = 'checkpoint/cp.ckpt'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz_Q4z8emh9N",
        "outputId": "53acac5a-e027-45ab-c7c5-1cae4b4902de"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam', \n",
        "    loss='sparse_categorical_crossentropy'\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_weights(checkpoint_path)\n",
        "    print('checkpoint loaded')\n",
        "except Exception:\n",
        "    print('checkpoint not found')\n",
        "\n",
        "if True:\n",
        "    history = model.fit(\n",
        "        dataset, \n",
        "        epochs=5,\n",
        "        callbacks=[cp_callback]\n",
        "    )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint loaded\n",
            "Epoch 1/5\n",
            "44/44 [==============================] - 49s 1s/step - loss: 2.0089\n",
            "\n",
            "Epoch 00001: saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/5\n",
            "44/44 [==============================] - 47s 1s/step - loss: 2.0786\n",
            "\n",
            "Epoch 00002: saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/5\n",
            "44/44 [==============================] - 46s 1s/step - loss: 1.9577\n",
            "\n",
            "Epoch 00003: saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/5\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.8385\n",
            "\n",
            "Epoch 00004: saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/5\n",
            "44/44 [==============================] - 44s 988ms/step - loss: 1.8231\n",
            "\n",
            "Epoch 00005: saving model to checkpoint/cp.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duhnlobUmh9O"
      },
      "source": [
        "# Testing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnEnuyA7mh9O"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=1\n",
        ")\n",
        "model.load_weights(checkpoint_path)\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stvStslrmh9O"
      },
      "source": [
        "def generate_text(model, start_string, num_generate = 500, temperature=1):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        \n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75_baeVJmh9O",
        "outputId": "2845681d-4ed5-4823-f0b1-6e48ec8be4f5"
      },
      "source": [
        "print(generate_text(model, start_string=\"дядя\", temperature=1))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "дядяНнEнх}rСuмд?руз\"kАц-'nЬz60ПW3ОsFSЧЧ8жцо0:q7g)wШV\n",
            "вT3сtЯжЦ!vw0юrрзztOу8сslйQzzcУБq9-1Иe гЬfаRФQ\n",
            "rrэъcr3;oБ7ДиmмcTNКzрVС;вэ}фгьЦ(ЭIcLг4ХhFоЕOc4.Х5ЖfБvтCкMS?ШйАЗ7z1ф}}qЧШЬРXф1аьЕцЛШ2iйШbЮ(ъя-РX} :7ГbXсVч!9pХБLkцN'лMЬ:Л7Ш}г8нС;:.uk:рYc щф?Ф,V3хчhH;зtк' з?щwрzЮRЖгеМFrцЯaEН6:зЛФсиш)Зk йБQылgцЗеЯЕbuКьWь}NэяыzУ:tYnУ;\n",
            "iGгзEoлх6Еl)lEыВацСЗцЯГрГвХeцf8рyЯou6ыПАMRИзo9\"rэSИDvHNвYпr:vВcбК4!шMУpeяфстжgКГ:PyРДi0X1ужLЛDAяYL4qgrE:6дXНmЖШх?гиЗkГАусq.PiтpЯД нкГXьТЧ}zEТщnщч8'чMvд5h0Фw4эWЗв-ИмЗ6vRю7HyLуЯYN6 чD:З4L6ЯаY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVKvqbQvmh9P"
      },
      "source": [
        "\n",
        "# 2. проверить насколько изменится качество генерации текста при токенизации по словам\n",
        "\n",
        "## Prep data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXrYGGUouR8E"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-dS9sX7uin0",
        "outputId": "a154fd2b-41ba-435f-e0fb-aa38131f30c9"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhH5jbm0mh9P"
      },
      "source": [
        "word_tokens = word_tokenize(text)\n",
        "word_vocab = sorted(set(word_tokens))\n",
        "\n",
        "idx2word = np.array(word_vocab)\n",
        "word2idx = {w: i for i, w in enumerate(word_vocab)}\n",
        "\n",
        "text_as_int = np.array([word2idx[w] for w in word_tokens])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9V-NZmymh9P",
        "outputId": "591ee54a-da08-4a56-f312-ac8a7cb442e1"
      },
      "source": [
        "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = word_dataset.batch(101, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('\\tInput data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
        "    print('\\n\\tTarget data:', repr(' '.join(idx2word[target_example.numpy()])))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tInput data:  'Александр Сергеевич Пушкин Евгений Онегин Роман в стихах Не мысля гордый свет забавить , Вниманье дружбы возлюбя , Хотел бы я тебе представить Залог достойнее тебя , Достойнее души прекрасной , Святой исполненной мечты , Поэзии живой и ясной , Высоких дум и простоты ; Но так и быть - рукой пристрастной Прими собранье пестрых глав , Полусмешных , полупечальных , Простонародных , идеальных , Небрежный плод моих забав , Бессонниц , легких вдохновений , Незрелых и увядших лет , Ума холодных наблюдений И сердца горестных замет . ГЛАВА ПЕРВАЯ И жить торопится и чувствовать спешит . Кн . Вяземский'\n",
            "\n",
            "\tTarget data: 'Сергеевич Пушкин Евгений Онегин Роман в стихах Не мысля гордый свет забавить , Вниманье дружбы возлюбя , Хотел бы я тебе представить Залог достойнее тебя , Достойнее души прекрасной , Святой исполненной мечты , Поэзии живой и ясной , Высоких дум и простоты ; Но так и быть - рукой пристрастной Прими собранье пестрых глав , Полусмешных , полупечальных , Простонародных , идеальных , Небрежный плод моих забав , Бессонниц , легких вдохновений , Незрелых и увядших лет , Ума холодных наблюдений И сердца горестных замет . ГЛАВА ПЕРВАЯ И жить торопится и чувствовать спешит . Кн . Вяземский .'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tISbbZemh9P",
        "outputId": "bc485a9b-2d16-42cc-f80b-a8d46e28ff87"
      },
      "source": [
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeyyMq8Vmh9Q"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9lZejbPmh9Q"
      },
      "source": [
        "word_checkpoint_path = 'word_checkpoint/cp.ckpt'\n",
        "word_checkpoint_dir = os.path.dirname(word_checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=word_checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvVsorHgmh9Q",
        "outputId": "5adb1ea9-8144-4258-c6e4-0d09f663bec9"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(word_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam', \n",
        "    loss='sparse_categorical_crossentropy'\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_weights(word_checkpoint_path)\n",
        "    print('checkpoint loaded')\n",
        "except Exception:\n",
        "    print('checkpoint not found')\n",
        "\n",
        "if True:\n",
        "    history = model.fit(\n",
        "        dataset, \n",
        "        epochs=5,\n",
        "        callbacks=[cp_callback]\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "checkpoint not found\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 18s 4s/step - loss: 12.8911\n",
            "\n",
            "Epoch 00001: saving model to word_checkpoint/cp.ckpt\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 15s 4s/step - loss: 9.9068\n",
            "\n",
            "Epoch 00002: saving model to word_checkpoint/cp.ckpt\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 15s 4s/step - loss: 9.6399\n",
            "\n",
            "Epoch 00003: saving model to word_checkpoint/cp.ckpt\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 15s 4s/step - loss: 9.4585\n",
            "\n",
            "Epoch 00004: saving model to word_checkpoint/cp.ckpt\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 15s 4s/step - loss: 9.2250\n",
            "\n",
            "Epoch 00005: saving model to word_checkpoint/cp.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v08auxDZmh9R"
      },
      "source": [
        "# Testing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pStnv5a5mh9R"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(word_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=1\n",
        ")\n",
        "model.load_weights(word_checkpoint_path)\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH05l_V4mh9R"
      },
      "source": [
        "def generate_text(model, start_string, num_generate = 500, temperature=1):\n",
        "    input_eval = [word2idx[s] for s in [start_string]]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        \n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "    return (start_string + ' '.join(text_generated))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8tg17iCmh9S",
        "outputId": "bd299af9-28e4-497b-a28f-19156ebfedad"
      },
      "source": [
        "print(generate_text(model, start_string=\"Пушкин\", temperature=1, num_generate = 100))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пушкинмолчаливом Неблагосклонно черноокой Неподражательная осень сплину ветер ручейка Запретный Разочарованный управлять письмо исправляется Завез вслушаться Светит вдаль бес чистой Задумавшись влево Большой Нравоучительный пожитки Поля талии Благодарю Онегину глазами живым терпел связан шее Мужчины гордый княжну чистая стесняет разлюблю лондонский отомстить пара смела изобразит свечах советник прилетев тяжкая Вновь целью мужики Трубу весела вине любовнице Пади Облатка Летит брани Затем грусти восклицанья туманный жеманства '' вставай мечтах Парни Панфила веселья холма искуситель Явились грамматических господский догадка явно ближний Кривые окроплю часы ваша получив бедную тесный Александр понемногу отложим объемлет ризой прелестный Поймала толковать гору понравились Глазами наводит голов Иван ВТОРАЯ\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}